{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5D Example \n",
    "\n",
    "In this notebook we will go through an example using the **foxi** code features to evaluate the expected utility of a mock scientific survey. This notebook will assume the reader is familiar with all of the concepts and is intended as a practical demonstration of the code. For more details about the utility functions themselves, as well as the inner workings of **foxi**, one can read the paper it was first developed in here: [https://arxiv.org/abs/1803.09491]. \n",
    "\n",
    "To begin with, we will import the module `sys` to append our path towards **foxi**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/Rob/work/foxi_train/foxisource/') # Give your directory to foxisource here.\n",
    "from foxi import foxi\n",
    "\n",
    "# These imports aren't stricly necessary to run foxi but they will be useful in our examples.\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of the current experiment\n",
    "\n",
    "Consider a scientific experiment over a 5D parameter space. Fitting a statistical model to the input, let us imagine that the posterior distribution is given by a multivariate Gaussian distribution with non-trivial covariances. Let us quickly generate $10^3$ samples from this 5D posterior. Feel free to change these settings to check the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a state of low information to start with.\n",
    "mock_posterior_best_fit = np.asarray([ 1.0, 2.0, 4.0, 0.5, 2.5]) \n",
    "mock_posterior_fisher_matrix = np.asarray([[ 2.0, 0.0, 0.0, 0.0, 0.0], \n",
    "                                           [ 0.0, 3.0, 0.0, 0.0, 0.0], # This obviously assumes that the  \n",
    "                                           [ 0.0, 0.0, 5.0, 0.0, 0.0], # covariance matrix is constant \n",
    "                                           [ 0.0, 0.0, 0.0, 4.0, 0.0], # with respect to the parameters.     \n",
    "                                           [ 0.0, 0.0, 0.0, 0.0, 2.0]])  \n",
    " \n",
    "# Quick inversion to generate the samples and mimic some weights too.        \n",
    "mock_posterior_covariance_matrix = np.linalg.inv(mock_posterior_fisher_matrix)\n",
    "mock_posterior_samples = np.random.multivariate_normal(mock_posterior_best_fit,mock_posterior_covariance_matrix,10**4)\n",
    "mock_posterior_sample_weights = multivariate_normal.pdf(mock_posterior_samples,mean=mock_posterior_best_fit,cov=mock_posterior_covariance_matrix)\n",
    "mock_posterior_samples_output = np.insert(mock_posterior_samples,0,mock_posterior_sample_weights,axis=1)\n",
    "\n",
    "# Let's output this data to a file in the '/foxichains' directory which mimics a real MCMC output.\n",
    "np.savetxt('/Users/Rob/work/foxi_train/foxichains/mock_posterior_samples.txt', mock_posterior_samples_output, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of the model priors\n",
    "\n",
    "We should now generate a series of toy model priors which can span the parameter space. These should represent a true collection of theoretical models but, in this case for simplicity and to capture the essential effects of the whole model space, we will choose `Nm` models that all have uniform hypersphere priors all with radius `Rm`, and positions drawn from a Gaussian hyperprior centred on `mock_posterior_best_fit`. Let us now quickly generate $100$ samples for each of these models and save them to files so that **foxi** can read them in. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to vary these (though consider the number of model pairs to compare grows like Nm*(Nm-1)/2).\n",
    "Nm = 10  \n",
    "Rm = 0.1\n",
    "    \n",
    "hyperprior_covariance_matrix = np.asarray([[ 0.05, 0.01, 0.01, 0.00, 0.00], \n",
    "                                           [ 0.01, 0.05, 0.01, 0.00, 0.00], # Different values here correspond to   \n",
    "                                           [ 0.01, 0.01, 0.05, 0.00, 0.00], # different model space alignments. \n",
    "                                           [ 0.00, 0.00, 0.00, 0.05, 0.00],      \n",
    "                                           [ 0.00, 0.00, 0.00, 0.00, 0.05]])  \n",
    "\n",
    "# Generate the positions.\n",
    "mock_prior_positions = np.random.multivariate_normal(mock_posterior_best_fit,hyperprior_covariance_matrix,Nm)\n",
    "\n",
    "# Generate the 5D hypersphere samples and output to text files in the '/foxipriors' directory.\n",
    "for im in range(0,Nm):\n",
    "    angle1 = np.random.uniform(0,np.pi,size=10**2)\n",
    "    angle2 = np.random.uniform(0,np.pi,size=10**2)\n",
    "    angle3 = np.random.uniform(0,np.pi,size=10**2)\n",
    "    angle4 = np.random.uniform(0,2.0*np.pi,size=10**2)\n",
    "    parameter1 = Rm*np.cos(angle1) + mock_prior_positions[im][0]\n",
    "    parameter2 = Rm*np.sin(angle1)*np.cos(angle2) + mock_prior_positions[im][1]\n",
    "    parameter3 = Rm*np.sin(angle1)*np.sin(angle2)*np.cos(angle3) + mock_prior_positions[im][2]\n",
    "    parameter4 = Rm*np.sin(angle1)*np.sin(angle2)*np.sin(angle3)*np.cos(angle4) + mock_prior_positions[im][3]\n",
    "    parameter5 = Rm*np.sin(angle1)*np.sin(angle2)*np.sin(angle3)*np.sin(angle4) + mock_prior_positions[im][4]\n",
    "    mock_prior_samples = np.asarray([parameter1,parameter2,parameter3,parameter4,parameter5]).T\n",
    "    np.savetxt('/Users/Rob/work/foxi_train/foxipriors/mock_prior' + str(im+1) + '_samples.txt', mock_prior_samples, delimiter='\\t')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of the forecast Fisher matrix\n",
    "\n",
    "Let us assume that the future likelihood will be a multivariate Gaussian over the parameters as well. The code can of course accomodate any specified fiduical point-dependent matrix, but it will be more instructive to simplify things in this way. In this instance, let us also model how the Fisher matrix varies with respect to the fiducial points with a polynomial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the polynomial baheviour of the forecast Fisher matrix with respect to the fiducial points.\n",
    "def fisher_matrix(fiducial_point):\n",
    "    return np.asarray([[2.0+abs(fiducial_point[0])**2.0, 0.0,0.0,0.0,0.0],\n",
    "                       [0.0,10.0+abs(fiducial_point[1])**2.0,0.0,0.0,0.0],\n",
    "                       [0.0,0.0,5.0+abs(fiducial_point[2])**2.0, 0.0,0.0],\n",
    "                       [0.0,0.0,0.0,10.0+abs(fiducial_point[3])**2.0,0.0],\n",
    "                       [0.0,0.0,0.0,0.0,10.0+abs(fiducial_point[4])**2.0]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the main foxi algorithm\n",
    "\n",
    "We are now ready to run **foxi** with our samples and forecast Fisher matrix. The first thing to do is to run a new instance of the `foxi` class like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "foxi_instance = foxi('/Users/Rob/work/foxi_train') # Give your directory to foxi here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to specify where our posterior samples of the current data are, how many samples to read in, what weights they carry and identify if there are any transformations to be done on each column (the latter will be no in this case). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "chains_filename = 'mock_posterior_samples.txt'\n",
    "\n",
    "# Note that the column numbers start from 0...\n",
    "parameter_column_numbers = [1,2,3,4,5]\n",
    "weights_column_number = 0\n",
    "\n",
    "# Simply set this to the number of samples generated - this can be useful to get results out as a sanity check.\n",
    "number_of_samples_to_read_in = 10**3\n",
    "\n",
    "foxi_instance.set_chains(chains_filename,\n",
    "                         parameter_column_numbers,\n",
    "                         number_of_samples_to_read_in,\n",
    "                         weights_column=weights_column_number, # All points are given weight 1 if this is ignored.\n",
    "                         column_types=None) # No transformations needed here. \n",
    "                                            # One could have ['flat','log10','log'] specified for each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the posterior chains have been set, we can do the same for our list of prior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the statsmodels module: http://www.statsmodels.org/stable/index.html\n",
      "The Kernel Density Bandwidth for each model listed in each dimension:\n",
      "\n",
      "Model 0: [0.04475697 0.03007876 0.02293838 0.01620533 0.01824696]\n",
      "Model 1: [0.04402931 0.03190598 0.02199352 0.01705286 0.01398627]\n",
      "Model 2: [0.04403217 0.03385921 0.02196357 0.01509964 0.01411397]\n",
      "Model 3: [0.04639548 0.03050075 0.02088313 0.01642939 0.01509258]\n",
      "Model 4: [0.04587787 0.0297153  0.02181327 0.0148354  0.01824378]\n",
      "Model 5: [0.04611706 0.03271811 0.01940892 0.01535048 0.01501017]\n",
      "Model 6: [0.04545002 0.03001187 0.02399223 0.01349952 0.01562673]\n",
      "Model 7: [0.04438425 0.03047156 0.02151398 0.01894682 0.01628939]\n",
      "Model 8: [0.04737934 0.03013409 0.02117865 0.01367327 0.01519263]\n",
      "Model 9: [0.0438121  0.03384669 0.01934215 0.01911765 0.01450829]\n"
     ]
    }
   ],
   "source": [
    "# List the model file names to compute the expected utilities for.\n",
    "model_name_list = ['mock_prior' + str(im+1) + '_samples.txt' for im in range(0,Nm)]\n",
    "\n",
    "# List the column numbers to use for each file of prior samples.\n",
    "prior_column_numbers = [[0,1,2,3,4] for im in range(0,Nm)]\n",
    "\n",
    "# Once again, simply set this to the number of samples we made earlier for each prior.\n",
    "number_of_prior_points_to_read_in = 10**2\n",
    "\n",
    "foxi_instance.set_model_name_list(model_name_list,\n",
    "                                  prior_column_numbers,\n",
    "                                  number_of_prior_points_to_read_in,\n",
    "                                  prior_column_types=None) # Once again, no transformations needed here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output here is from a quick KDE of each set of prior samples. How these are used depends on the specific numerical situation, which is once again covered in more detail in [https://arxiv.org/abs/1803.09491]. Our final step before running **foxi** is to give it a python function which returns the forecast Fisher matrix at each fiducial point (i.e. evaluated at each point of the current data chains). We do this like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "foxi_instance.set_fisher_matrix(fisher_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the necessary settings have now been applied for a full run of the **foxi** code. Depending on how many points were introduced at the level of the priors and chains, as well as the length of time it takes to evaluate the forecast Fisher matrix at each fiducial point, we may wish to continue on a compute cluster. For this simple example we have chosen, it should be enough to simply run locally on a laptop in less than 10 minutes. Our main run command is the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          \n",
      ">>>>>>>>>>>                               \n",
      ">>>       >>                              \n",
      ">>                                        \n",
      ">>                                  >>    \n",
      ">>>>>>>>>    >>>>>>>    >>>    >>>        \n",
      ">>          >>     >>     >>  >>    >>    \n",
      ">>         >>       >>     >>>>     >>    \n",
      ">>         >>       >>    >>>>      >>    \n",
      ">>          >>     >>    >>  >>     >>    \n",
      ">>           >>>>>>>   >>>    >>>   >>>   \n",
      "                                          \n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "       Author: Robert J. Hardwick         \n",
      "      DISTRIBUTED UNDER MIT LICENSE       \n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "                                          \n",
      "    NOW WORKING WITH FISHER FORECASTS     \n",
      "                                          \n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "Number of maxed-out evidences for each model:\n",
      "\n",
      "Model 0: 0\n",
      "Model 1: 0\n",
      "Model 2: 0\n",
      "Model 3: 0\n",
      "Model 4: 0\n",
      "Model 5: 0\n",
      "Model 6: 0\n",
      "Model 7: 0\n",
      "Model 8: 0\n",
      "Model 9: 0\n"
     ]
    }
   ],
   "source": [
    "mix_models = True # Set this to 'True' so that the expected utilities for all possible model pairs are calculated.\n",
    "                  # The default is 'False' which calculates the utilities all with respect to the reference model\n",
    "                  # here the 0-element in 'model_name_list'.\n",
    "        \n",
    "foxi_instance.run_foxifish(mix_models=mix_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't be too alarmed by a few error messages about dividing by zero: these occur infrequently due to excessively low densities appearing in the numerical procedure but are automatically discarded as points from the main result. The algorithm should terminate with a `Number of maxed-out evidences for each model:` message. This message refers to the number of times when the model was found to be ruled out at the level of its maximum likelihood. \n",
    "\n",
    "### Post-processing and other foxi features\n",
    "\n",
    "This last step was the longest running, from now onwards the post-processing of the results from **foxi** will always be quick enough to run locally on a laptop. The next step is to use `rerun_foxi` to analyse the output that `run_foxifish` dumped into the `/foxioutput` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          \n",
      ">>>>>>>>>>>                               \n",
      ">>>       >>                              \n",
      ">>                                        \n",
      ">>                                  >>    \n",
      ">>>>>>>>>    >>>>>>>    >>>    >>>        \n",
      ">>          >>     >>     >>  >>    >>    \n",
      ">>         >>       >>     >>>>     >>    \n",
      ">>         >>       >>    >>>>      >>    \n",
      ">>          >>     >>    >>  >>     >>    \n",
      ">>           >>>>>>>   >>>    >>>   >>>   \n",
      "                                          \n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "       Author: Robert J. Hardwick         \n",
      "      DISTRIBUTED UNDER MIT LICENSE       \n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "                                          \n",
      "Just read in a `mix_models' foxiplot data file with data dimension 5 and the number of model pairs is 55\n"
     ]
    }
   ],
   "source": [
    "foxiplot_file_name = 'foxiplots_data_mix_models.txt' # This is the generic name that 'run_foxifish' will set, change\n",
    "                                                     # this to whatever you like as long as the file is in '/foxioutput'.\n",
    "                                                     # If 'mix_models = False' then remove the 'mix_models' tag.\n",
    "\n",
    "# Set this to the number of samples generated - useful to vary to check convergence though we will make plots later.\n",
    "number_of_foxiplot_samples_to_read_in = 10**3 \n",
    "\n",
    "# We must set this feature to 'flat' in each column to perform no post-processing transformation that reweights the chains.\n",
    "# This can be a little redundant as it makes more numerical sense to simply generate new chains. \n",
    "post_chains_column_types = ['flat','flat','flat','flat','flat']\n",
    "\n",
    "# Set this to 'True' for the output to include fully-formatted LaTeX tables!\n",
    "TeX_output = True\n",
    "\n",
    "# For the truly lazy - you can set the TeX name for each model in the table output too.\n",
    "model_TeX_names = [r'${\\cal M}_' + str(i) + r'$' for i in range(0,Nm)]\n",
    "\n",
    "foxi_instance.rerun_foxi(foxiplot_file_name,number_of_foxiplot_samples_to_read_in,post_chains_column_types,model_name_TeX_input=model_TeX_names,TeX_output=TeX_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, don't be too alarmed by a few value error messages. If there are any error messages related to converting strings to floats, this is likely to do with a delimiter problem in the text files that were read in. Tab delimited data should work fine (or at least give a number of spaces). Note also that the number of model pairs quoted here is actually the number of unique pairs + the auto-pairs (i.e. Model_i - Model_j as well as Model_i - Model_i) so this number is an additional `+N_m` larger than the number of unique pairs. A summary of the main results can be read from this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [<|lnB|>_1, <|lnB|>_2, ...] = [0.         7.13949588 3.54252784 1.96244785 3.11499612 4.48519101\n",
      " 3.38517049 5.45135066 4.47040482 4.80322344 0.         6.89323786\n",
      " 6.42265575 5.74799757 4.08894828 5.88328596 6.19814586 4.9617276\n",
      " 4.31835716 0.         2.06543351 4.85277293 4.38120967 3.81736211\n",
      " 5.86051277 4.98013573 3.99211372 0.         3.39313949 3.97178291\n",
      " 3.24042666 5.45303046 4.12438734 3.91150725 0.         5.26961806\n",
      " 5.18446365 7.36248621 5.76247833 5.66165391 0.         2.29951833\n",
      " 3.48347557 2.9438242  1.31189337 0.         3.5882871  3.42725462\n",
      " 2.43612895 0.         2.4016791  3.74371312 0.         3.01221195\n",
      " 0.        ]\n",
      " [DECI_1, DECI_2, ...] = [0.    0.551 0.247 0.054 0.169 0.34  0.225 0.426 0.346 0.374 0.    0.52\n",
      " 0.505 0.459 0.309 0.477 0.498 0.398 0.341 0.    0.078 0.394 0.311 0.258\n",
      " 0.458 0.384 0.297 0.    0.227 0.284 0.21  0.435 0.295 0.289 0.    0.431\n",
      " 0.441 0.566 0.467 0.452 0.    0.097 0.233 0.172 0.005 0.    0.248 0.245\n",
      " 0.102 0.    0.105 0.277 0.    0.18  0.   ]\n",
      "<DKL> = 3.85946246771\n",
      " [<|lnB|_ML>_1, <|lnB|_ML>_2, ...] = [0.         5.56833307 2.53218028 1.25451519 1.8447548  3.20232526\n",
      " 2.34133312 3.99938082 3.21628616 3.58149397 0.         5.29177194\n",
      " 4.94113253 4.50857038 2.78558848 4.3192743  4.27523475 3.635739\n",
      " 2.91202816 0.         1.2729173  3.33250992 3.06996045 2.64703507\n",
      " 4.52149436 3.71076885 2.57039054 0.         2.06395836 2.91247434\n",
      " 2.23738204 4.0311583  2.952352   2.76295668 0.         3.80202359\n",
      " 3.54187084 5.55265686 4.0954425  4.17049194 0.         1.38669079\n",
      " 2.23398085 2.53852153 0.8967337  0.         2.71134109 2.95622579\n",
      " 1.56861672 0.         1.47709515 2.56734044 0.         2.56849108\n",
      " 0.        ]\n",
      " [r_ML|_1, r_ML|_2, ...] = [0.785 0.626 0.72  0.724 0.726 0.658 0.676 0.655 0.669 0.649 0.76  0.649\n",
      " 0.623 0.635 0.652 0.614 0.625 0.639 0.651 0.799 0.741 0.705 0.67  0.679\n",
      " 0.651 0.671 0.684 0.759 0.703 0.649 0.663 0.635 0.661 0.654 0.776 0.636\n",
      " 0.636 0.617 0.639 0.627 0.724 0.672 0.656 0.655 0.692 0.726 0.648 0.641\n",
      " 0.668 0.752 0.698 0.651 0.748 0.656 0.729]\n",
      " [< (|lnB|_1 - <|lnB|>_1)^2 >, < (|lnB|_2 - <|lnB|>_2)^2 >, ...] = [ 0.         36.0578692   9.51636556  2.67063516  8.08004926 15.66905963\n",
      "  7.80247575 21.54244145 16.9283293  18.0811578   0.         37.24705943\n",
      " 29.5985233  25.53077648 11.37283508 25.46540144 31.9462923  17.3440247\n",
      " 13.29011357  0.          3.954719   18.0502433  17.19789105 12.77461516\n",
      " 26.97151857 21.97815609 13.85845509  0.          8.0222047  12.64317836\n",
      "  7.01898688 21.35786148 13.64455955 11.48996727  0.         19.4011653\n",
      " 17.81566173 37.16601887 22.54611064 22.87219569  0.          3.85927189\n",
      "  8.98689026  5.76875298  1.1152249   0.          8.40355792  8.15975045\n",
      "  4.26230877  0.          4.23996003  9.34460542  0.          6.47709364\n",
      "  0.        ]\n",
      " [< (|lnB|_1 - <|lnB|>_1_ML)^2 >_ML, < (|lnB|_2 - <|lnB|>_2_ML)^2 >_ML, ...] = [ 0.         13.54894274  3.1809097   0.7264751   1.41697942  5.17415138\n",
      "  2.66990815  6.92436094  4.82692041  6.25454603  0.         13.99351273\n",
      " 11.42824963  9.31795792  3.60463425  7.97450619  8.8877086   6.2971802\n",
      "  4.10214672  0.          0.94182266  6.06911497  4.82592468  3.8456244\n",
      "  8.25670498  5.93486695  3.8585971   0.          2.01978568  4.53679279\n",
      "  2.88848467  6.92201851  3.63575925  4.11425655  0.          6.7583826\n",
      "  5.57382301 12.93996036  7.52561719  8.20819402  0.          0.92676602\n",
      "  2.15336039  3.20937095  0.44899665  0.          3.04097204  4.19448513\n",
      "  1.36189203  0.          0.99463731  2.91466903  0.          3.27618435\n",
      "  0.        ]\n",
      " [DECI_1|_ML, DECI_2|_ML, ...] = [0.         0.49197861 0.12142857 0.         0.01459854 0.21052632\n",
      " 0.07407407 0.31304348 0.20543807 0.27635328 0.         0.46438746\n",
      " 0.43766578 0.40273973 0.14655172 0.39378238 0.368      0.27146814\n",
      " 0.14899713 0.         0.003861   0.26779661 0.18181818 0.12461059\n",
      " 0.3982808  0.29179331 0.13607595 0.         0.02693603 0.16809117\n",
      " 0.08011869 0.32328767 0.16519174 0.1416185  0.         0.31043956\n",
      " 0.27197802 0.50130548 0.34072022 0.34852547 0.         0.00304878\n",
      " 0.04651163 0.10724638 0.         0.         0.11079545 0.17270195\n",
      " 0.0060241  0.         0.         0.09169054 0.         0.09593023\n",
      " 0.        ]\n",
      "< (DKL-<DKL>)^2 > = 1.49550307095\n"
     ]
    }
   ],
   "source": [
    "print(open('/Users/Rob/work/foxi_train/foxioutput/foxiplots_data_summary.txt', 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to avoid in these results is if the quoted expected Kullback-Leibler divergence `<DKL>` is of roughly the same value as the natural log of the number of points in the posterior chains - which in this example is $\\ln (10^3) \\simeq 6.9$. More chain points must be used for larger values since otherwise the value of `<DKL>` is only a lower bound. \n",
    "\n",
    "A nice feature, if one sets `TeX_output = True` in `rerun_foxi` above, is the output of a fully-formatted $\\LaTeX$ table incorporating of all of these results. This can be found in the following file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\centering\n",
      "\\begin{tabular}{|c|c|c|c|c|c|}\n",
      "\\cline{2-6}\n",
      "\\multicolumn{1}{c}{\\cellcolor{red!55}} & \\multicolumn{5}{|c|}{Data Name}  \\\\      \\hline\n",
      "\\backslashbox{${\\cal M}_\\beta$ - ${\\cal M}_\\gamma$}{$\\langle  U\\rangle$}  & $\\langle \\vert \\ln {\\rm B}_{\\beta \\gamma}\\vert \\rangle$ & $\\langle \\vert \\ln {\\rm B}_{\\beta \\gamma} \\vert \\rangle_{{}_{\\rm ML}}$ & $\\mathscr{D}_{\\beta \\gamma}$ & $\\mathscr{D}_{\\beta \\gamma}\\vert_{{}_{\\rm ML}}$ & $r_{{}_\\mathrm{ML}}$ \\\\     \\hline\\hline\n",
      "${\\cal M}_0$ - ${\\cal M}_0$ & $0.0^{+0.0}_{-0.0}$ & $0.0^{+0.0}_{-0.0}$ & 0.0 & 0.0 & 0.79  \\\\      \\hline\n",
      "${\\cal M}_1$ - ${\\cal M}_0$ & 7.14 $\\pm$ 6.0 & 5.57 $\\pm$ 3.68 & 0.55 & 0.49 & 0.63  \\\\      \\hline\n",
      "${\\cal M}_2$ - ${\\cal M}_0$ & 3.54 $\\pm$ 3.08 & 2.53 $\\pm$ 1.78 & 0.25 & 0.12 & 0.72  \\\\      \\hline\n",
      "${\\cal M}_3$ - ${\\cal M}_0$ & 1.96 $\\pm$ 1.63 & 1.25 $\\pm$ 0.85 & 0.05 & 0.0 & 0.72  \\\\      \\hline\n",
      "${\\cal M}_4$ - ${\\cal M}_0$ & 3.11 $\\pm$ 2.84 & 1.84 $\\pm$ 1.19 & 0.17 & 0.01 & 0.73  \\\\      \\hline\n",
      "${\\cal M}_5$ - ${\\cal M}_0$ & 4.49 $\\pm$ 3.96 & 3.2 $\\pm$ 2.27 & 0.34 & 0.21 & 0.66  \\\\      \\hline\n",
      "${\\cal M}_6$ - ${\\cal M}_0$ & 3.39 $\\pm$ 2.79 & 2.34 $\\pm$ 1.63 & 0.23 & 0.07 & 0.68  \\\\      \\hline\n",
      "${\\cal M}_7$ - ${\\cal M}_0$ & 5.45 $\\pm$ 4.64 & 4.0 $\\pm$ 2.63 & 0.43 & 0.31 & 0.66  \\\\      \\hline\n",
      "${\\cal M}_8$ - ${\\cal M}_0$ & 4.47 $\\pm$ 4.11 & 3.22 $\\pm$ 2.2 & 0.35 & 0.21 & 0.67  \\\\      \\hline\n",
      "${\\cal M}_9$ - ${\\cal M}_0$ & 4.8 $\\pm$ 4.25 & 3.58 $\\pm$ 2.5 & 0.37 & 0.28 & 0.65  \\\\      \\hline\n",
      "${\\cal M}_1$ - ${\\cal M}_1$ & $0.0^{+0.0}_{-0.0}$ & $0.0^{+0.0}_{-0.0}$ & 0.0 & 0.0 & 0.76  \\\\      \\hline\n",
      "${\\cal M}_2$ - ${\\cal M}_1$ & 6.89 $\\pm$ 6.1 & 5.29 $\\pm$ 3.74 & 0.52 & 0.46 & 0.65  \\\\      \\hline\n",
      "${\\cal M}_3$ - ${\\cal M}_1$ & 6.42 $\\pm$ 5.44 & 4.94 $\\pm$ 3.38 & 0.51 & 0.44 & 0.62  \\\\      \\hline\n",
      "${\\cal M}_4$ - ${\\cal M}_1$ & 5.75 $\\pm$ 5.05 & 4.51 $\\pm$ 3.05 & 0.46 & 0.4 & 0.64  \\\\      \\hline\n",
      "${\\cal M}_5$ - ${\\cal M}_1$ & 4.09 $\\pm$ 3.37 & 2.79 $\\pm$ 1.9 & 0.31 & 0.15 & 0.65  \\\\      \\hline\n",
      "${\\cal M}_6$ - ${\\cal M}_1$ & 5.88 $\\pm$ 5.05 & 4.32 $\\pm$ 2.82 & 0.48 & 0.39 & 0.61  \\\\      \\hline\n",
      "${\\cal M}_7$ - ${\\cal M}_1$ & 6.2 $\\pm$ 5.65 & 4.28 $\\pm$ 2.98 & 0.5 & 0.37 & 0.63  \\\\      \\hline\n",
      "${\\cal M}_8$ - ${\\cal M}_1$ & 4.96 $\\pm$ 4.16 & 3.64 $\\pm$ 2.51 & 0.4 & 0.27 & 0.64  \\\\      \\hline\n",
      "${\\cal M}_9$ - ${\\cal M}_1$ & 4.32 $\\pm$ 3.65 & 2.91 $\\pm$ 2.03 & 0.34 & 0.15 & 0.65  \\\\      \\hline\n",
      "${\\cal M}_2$ - ${\\cal M}_2$ & $0.0^{+0.0}_{-0.0}$ & $0.0^{+0.0}_{-0.0}$ & 0.0 & 0.0 & 0.8  \\\\      \\hline\n",
      "${\\cal M}_3$ - ${\\cal M}_2$ & 2.07 $\\pm$ 1.99 & 1.27 $\\pm$ 0.97 & 0.08 & 0.0 & 0.74  \\\\      \\hline\n",
      "${\\cal M}_4$ - ${\\cal M}_2$ & 4.85 $\\pm$ 4.25 & 3.33 $\\pm$ 2.46 & 0.39 & 0.27 & 0.7  \\\\      \\hline\n",
      "${\\cal M}_5$ - ${\\cal M}_2$ & 4.38 $\\pm$ 4.15 & 3.07 $\\pm$ 2.2 & 0.31 & 0.18 & 0.67  \\\\      \\hline\n",
      "${\\cal M}_6$ - ${\\cal M}_2$ & 3.82 $\\pm$ 3.57 & 2.65 $\\pm$ 1.96 & 0.26 & 0.12 & 0.68  \\\\      \\hline\n",
      "${\\cal M}_7$ - ${\\cal M}_2$ & 5.86 $\\pm$ 5.19 & 4.52 $\\pm$ 2.87 & 0.46 & 0.4 & 0.65  \\\\      \\hline\n",
      "${\\cal M}_8$ - ${\\cal M}_2$ & 4.98 $\\pm$ 4.69 & 3.71 $\\pm$ 2.44 & 0.38 & 0.29 & 0.67  \\\\      \\hline\n",
      "${\\cal M}_9$ - ${\\cal M}_2$ & 3.99 $\\pm$ 3.72 & 2.57 $\\pm$ 1.96 & 0.3 & 0.14 & 0.68  \\\\      \\hline\n",
      "${\\cal M}_3$ - ${\\cal M}_3$ & $0.0^{+0.0}_{-0.0}$ & $0.0^{+0.0}_{-0.0}$ & 0.0 & 0.0 & 0.76  \\\\      \\hline\n",
      "${\\cal M}_4$ - ${\\cal M}_3$ & 3.39 $\\pm$ 2.83 & 2.06 $\\pm$ 1.42 & 0.23 & 0.03 & 0.7  \\\\      \\hline\n",
      "${\\cal M}_5$ - ${\\cal M}_3$ & 3.97 $\\pm$ 3.56 & 2.91 $\\pm$ 2.13 & 0.28 & 0.17 & 0.65  \\\\      \\hline\n",
      "${\\cal M}_6$ - ${\\cal M}_3$ & 3.24 $\\pm$ 2.65 & 2.24 $\\pm$ 1.7 & 0.21 & 0.08 & 0.66  \\\\      \\hline\n",
      "${\\cal M}_7$ - ${\\cal M}_3$ & 5.45 $\\pm$ 4.62 & 4.03 $\\pm$ 2.63 & 0.43 & 0.32 & 0.64  \\\\      \\hline\n",
      "${\\cal M}_8$ - ${\\cal M}_3$ & 4.12 $\\pm$ 3.69 & 2.95 $\\pm$ 1.91 & 0.29 & 0.17 & 0.66  \\\\      \\hline\n",
      "${\\cal M}_9$ - ${\\cal M}_3$ & 3.91 $\\pm$ 3.39 & 2.76 $\\pm$ 2.03 & 0.29 & 0.14 & 0.65  \\\\      \\hline\n",
      "${\\cal M}_4$ - ${\\cal M}_4$ & $0.0^{+0.0}_{-0.0}$ & $0.0^{+0.0}_{-0.0}$ & 0.0 & 0.0 & 0.78  \\\\      \\hline\n",
      "${\\cal M}_5$ - ${\\cal M}_4$ & 5.27 $\\pm$ 4.4 & 3.8 $\\pm$ 2.6 & 0.43 & 0.31 & 0.64  \\\\      \\hline\n",
      "${\\cal M}_6$ - ${\\cal M}_4$ & 5.18 $\\pm$ 4.22 & 3.54 $\\pm$ 2.36 & 0.44 & 0.27 & 0.64  \\\\      \\hline\n",
      "${\\cal M}_7$ - ${\\cal M}_4$ & 7.36 $\\pm$ 6.1 & 5.55 $\\pm$ 3.6 & 0.57 & 0.5 & 0.62  \\\\      \\hline\n",
      "${\\cal M}_8$ - ${\\cal M}_4$ & 5.76 $\\pm$ 4.75 & 4.1 $\\pm$ 2.74 & 0.47 & 0.34 & 0.64  \\\\      \\hline\n",
      "${\\cal M}_9$ - ${\\cal M}_4$ & 5.66 $\\pm$ 4.78 & 4.17 $\\pm$ 2.86 & 0.45 & 0.35 & 0.63  \\\\      \\hline\n",
      "${\\cal M}_5$ - ${\\cal M}_5$ & $0.0^{+0.0}_{-0.0}$ & $0.0^{+0.0}_{-0.0}$ & 0.0 & 0.0 & 0.72  \\\\      \\hline\n",
      "${\\cal M}_6$ - ${\\cal M}_5$ & 2.3 $\\pm$ 1.96 & 1.39 $\\pm$ 0.96 & 0.1 & 0.0 & 0.67  \\\\      \\hline\n",
      "${\\cal M}_7$ - ${\\cal M}_5$ & 3.48 $\\pm$ 3.0 & 2.23 $\\pm$ 1.47 & 0.23 & 0.05 & 0.66  \\\\      \\hline\n",
      "${\\cal M}_8$ - ${\\cal M}_5$ & 2.94 $\\pm$ 2.4 & 2.54 $\\pm$ 1.79 & 0.17 & 0.11 & 0.66  \\\\      \\hline\n",
      "${\\cal M}_9$ - ${\\cal M}_5$ & 1.31 $\\pm$ 1.06 & 0.9 $\\pm$ 0.67 & 0.01 & 0.0 & 0.69  \\\\      \\hline\n",
      "${\\cal M}_6$ - ${\\cal M}_6$ & $0.0^{+0.0}_{-0.0}$ & $0.0^{+0.0}_{-0.0}$ & 0.0 & 0.0 & 0.73  \\\\      \\hline\n",
      "${\\cal M}_7$ - ${\\cal M}_6$ & 3.59 $\\pm$ 2.9 & 2.71 $\\pm$ 1.74 & 0.25 & 0.11 & 0.65  \\\\      \\hline\n",
      "${\\cal M}_8$ - ${\\cal M}_6$ & 3.43 $\\pm$ 2.86 & 2.96 $\\pm$ 2.05 & 0.24 & 0.17 & 0.64  \\\\      \\hline\n",
      "${\\cal M}_9$ - ${\\cal M}_6$ & 2.44 $\\pm$ 2.06 & 1.57 $\\pm$ 1.17 & 0.1 & 0.01 & 0.67  \\\\      \\hline\n",
      "${\\cal M}_7$ - ${\\cal M}_7$ & $0.0^{+0.0}_{-0.0}$ & $0.0^{+0.0}_{-0.0}$ & 0.0 & 0.0 & 0.75  \\\\      \\hline\n",
      "${\\cal M}_8$ - ${\\cal M}_7$ & 2.4 $\\pm$ 2.06 & 1.48 $\\pm$ 1.0 & 0.1 & 0.0 & 0.7  \\\\      \\hline\n",
      "${\\cal M}_9$ - ${\\cal M}_7$ & 3.74 $\\pm$ 3.06 & 2.57 $\\pm$ 1.71 & 0.28 & 0.09 & 0.65  \\\\      \\hline\n",
      "${\\cal M}_8$ - ${\\cal M}_8$ & $0.0^{+0.0}_{-0.0}$ & $0.0^{+0.0}_{-0.0}$ & 0.0 & 0.0 & 0.75  \\\\      \\hline\n",
      "${\\cal M}_9$ - ${\\cal M}_8$ & 3.01 $\\pm$ 2.55 & 2.57 $\\pm$ 1.81 & 0.18 & 0.1 & 0.66  \\\\      \\hline\n",
      "${\\cal M}_9$ - ${\\cal M}_9$ & $0.0^{+0.0}_{-0.0}$ & $0.0^{+0.0}_{-0.0}$ & 0.0 & 0.0 & 0.73  \\\\      \\hline\n",
      "\\end{tabular}\n",
      "\\caption{Blah blah blah table...}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(open('/Users/Rob/work/foxi_train/foxioutput/foxiplots_data_summary_TeX.txt', 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful tool provided by **foxi** is to generate plots illustrating the numerical convergence (or lack thereof) of the calculated expected utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply specify the number of bins in which to re-calculate the expected utilty with more samples. \n",
    "number_of_bins = 50\n",
    "\n",
    "# We have already specified the other inputs here so let's simply generate the plots!\n",
    "foxi_instance.plot_convergence(foxiplot_file_name,number_of_bins,number_of_foxiplot_samples_to_read_in,post_chains_column_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The output can be found in `/foxiplots/foxiplot_convergence.pdf`.\n",
    "\n",
    "### Computing analytic expected utility estimates with foxi\n",
    "\n",
    "In the regime where the number of samples from the current posterior is large enough (i.e. such that the expected Kullback-Leibler divergence is less than $\\ln (\\#_{\\rm samples})$) and the prior volume of each model is small enough to be effectively described as a single point in parameter space (in the present example this is modelled with `Rm` being small), the expected utilities `<DKL>` and `<|lnB|>` (with their standard deviations) can be compared with analytic estimates of the same quantities. These formulae rely on knowledge of the Fisher estimate for the current posterior and are built into **foxi**. One can obtain them through using the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DKL> estimate: 2.762575144483158\n",
      "Sum_i <|lnB|>_i estimate: 107.21283488738516\n",
      "<|lnB|>_0 estimate: 3.0846895430056622\n",
      "<|lnB|>_1 estimate: 1.6630932970048868\n",
      "<|lnB|>_2 estimate: 1.2697094930439952\n",
      "<|lnB|>_3 estimate: 1.4314272313113006\n",
      "<|lnB|>_4 estimate: 2.711081199457574\n",
      "<|lnB|>_5 estimate: 2.231660640226594\n",
      "<|lnB|>_6 estimate: 2.166398840619568\n",
      "<|lnB|>_7 estimate: 2.0501646182900775\n",
      "<|lnB|>_8 estimate: 2.528960770375072\n",
      "<|lnB|>_9 estimate: 3.260917809550255\n",
      "<|lnB|>_10 estimate: 3.3873423886539005\n",
      "<|lnB|>_11 estimate: 2.0088919703992003\n",
      "<|lnB|>_12 estimate: 3.301920472382519\n",
      "<|lnB|>_13 estimate: 3.5341405462830005\n",
      "<|lnB|>_14 estimate: 2.03660482752888\n",
      "<|lnB|>_15 estimate: 2.3510589134622797\n",
      "<|lnB|>_16 estimate: 3.189378705439924\n",
      "<|lnB|>_17 estimate: 1.5290931484273684\n",
      "<|lnB|>_18 estimate: 2.756999412755398\n",
      "<|lnB|>_19 estimate: 2.9161082312475686\n",
      "<|lnB|>_20 estimate: 2.986223996679991\n",
      "<|lnB|>_21 estimate: 2.7437074203045064\n",
      "<|lnB|>_22 estimate: 2.6646327021513505\n",
      "<|lnB|>_23 estimate: 2.7830615033050794\n",
      "<|lnB|>_24 estimate: 2.3936506981599286\n",
      "<|lnB|>_25 estimate: 2.0354578592609562\n",
      "<|lnB|>_26 estimate: 1.8741312554442344\n",
      "<|lnB|>_27 estimate: 2.8021332115097266\n",
      "<|lnB|>_28 estimate: 1.7396183455973946\n",
      "<|lnB|>_29 estimate: 1.9536263487183052\n",
      "<|lnB|>_30 estimate: 3.897973347224548\n",
      "<|lnB|>_31 estimate: 3.219108074788657\n",
      "<|lnB|>_32 estimate: 3.1310627910733118\n",
      "<|lnB|>_33 estimate: 3.051988072920156\n",
      "<|lnB|>_34 estimate: 3.7902447240809494\n",
      "<|lnB|>_35 estimate: 1.354129433986958\n",
      "<|lnB|>_36 estimate: 2.6536210144638854\n",
      "<|lnB|>_37 estimate: 1.8442652729996443\n",
      "<|lnB|>_38 estimate: 0.5898819017940546\n",
      "<|lnB|>_39 estimate: 2.2510455719608347\n",
      "<|lnB|>_40 estimate: 1.5606695280608225\n",
      "<|lnB|>_41 estimate: 1.1360606119424839\n",
      "<|lnB|>_42 estimate: 1.4094236544411456\n",
      "<|lnB|>_43 estimate: 2.4983256236040265\n",
      "<|lnB|>_44 estimate: 1.4391498634471958\n"
     ]
    }
   ],
   "source": [
    "stepsizes = [np.sqrt(mock_posterior_covariance_matrix[0,0]),\n",
    "             np.sqrt(mock_posterior_covariance_matrix[1,1]), # Set the stepsizes in calculating the \n",
    "             np.sqrt(mock_posterior_covariance_matrix[2,2]), # variation of the forecast Fisher matrix\n",
    "             np.sqrt(mock_posterior_covariance_matrix[3,3]), # with respect to the fiducial points.\n",
    "             np.sqrt(mock_posterior_covariance_matrix[4,4])]\n",
    "\n",
    "# The rest of the inputs have been determined in our example previously\n",
    "foxi_instance.analytic_estimates(mock_prior_positions,mock_posterior_best_fit,mock_posterior_fisher_matrix,stepsizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agreement between the analytic estimates and the output from **foxi** can typically be improved by reducing `Rm` and increasing the number of points drawn from the current posterior, though the estimates have limited accuracy themeselves depending on the number of derivatives that can be taken in the forecast Fisher matrix. The comparison between these separate results can be a useful guide when obtaining forecasts from **foxi** to the required accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
